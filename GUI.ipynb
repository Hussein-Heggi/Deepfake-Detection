{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, ttk\n",
    "import pyaudio\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import torchvision.models as models\n",
    "from transformers import HubertModel, AutoProcessor\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerFeatureDataset(Dataset):\n",
    "    def __init__(self, pkl_path):\n",
    "        data = pickle.load(open(pkl_path, 'rb'))\n",
    "        self.X = torch.tensor(data['features'], dtype=torch.float32)  \n",
    "        self.y = torch.tensor(data['labels'],   dtype=torch.long)    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "class LayerWeightedAggregator(nn.Module):\n",
    "    def __init__(self, num_layers):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.ones(num_layers) / num_layers)\n",
    "    def forward(self, x):\n",
    "        w = torch.softmax(self.w, dim=0)             \n",
    "        return (x * w[None, :, None]).sum(dim=1)\n",
    "\n",
    "class ResNetClassifier(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.agg = LayerWeightedAggregator(num_layers)\n",
    "        H = W = int(np.sqrt(hidden_dim))\n",
    "        assert H*W == hidden_dim, \"hidden_dim must be square\"\n",
    "        self.H, self.W = H, W\n",
    "\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(1,\n",
    "            self.resnet.conv1.out_channels,\n",
    "            kernel_size=self.resnet.conv1.kernel_size,\n",
    "            stride=self.resnet.conv1.stride,\n",
    "            padding=self.resnet.conv1.padding,\n",
    "            bias=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.agg(x)               \n",
    "        b = x.size(0)\n",
    "        x = x.view(b, 1, self.H, self.W)\n",
    "        return self.resnet(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/AML/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/g7/Desktop/AML/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Sampled Files: ['../ASV/Data/LA_E_6189894.flac', '../ASV/Artifact/LA_E_9638498_filter.wav', '../ASV/Artifact/LA_E_1096928_filter.wav', '../ASV/Data/LA_E_6346279.flac', '../ASV/Artifact/LA_E_5241439_clipping.wav', '../ASV/Artifact/LA_E_2091174_clipping.wav', '../ASV/Data/LA_E_5064331.flac', '../ASV/Artifact/LA_E_5960975_compression.wav', '../ASV/Data/LA_E_8816429.flac', '../ASV/Artifact/LA_E_5167557_clipping.wav']\n",
      "Sampled Labels: [1, 0, 0, 1, 0, 1, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "pkl_path = \"../Features/Opt_Features/ASV_Opt_Dataset.pkl\"\n",
    "dataset = LayerFeatureDataset(pkl_path)\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_layers  = dataset.X.size(1)\n",
    "hidden_dim  = dataset.X.size(2)\n",
    "num_classes = len(torch.unique(dataset.y))\n",
    "\n",
    "model = ResNetClassifier(num_layers, hidden_dim, num_classes)\n",
    "model.load_state_dict(torch.load(\"../model.pth\"))\n",
    "model.eval()\n",
    "hub = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\", output_hidden_states=True).to(device)\n",
    "hub.eval()\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "with open(\"../ASV_sample.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(\"Sampled Files:\", data[\"files\"])\n",
    "print(\"Sampled Labels:\", data[\"labels\"])\n",
    "\n",
    "audio_files = data[\"files\"]\n",
    "audio_labels = data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(filename):\n",
    "    waveform, sample_rate = torchaudio.load(filename)\n",
    "    waveform = waveform.to(device)\n",
    "    if sample_rate != 16000:\n",
    "        waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)\n",
    "    if waveform.size(0) > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    return waveform.squeeze(0)\n",
    "\n",
    "\n",
    "def extract_features(waveform):\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\").to(device)\n",
    "        outputs = hub(**inputs)\n",
    "        hidden_states = outputs.hidden_states[2:6]  # layers 3rd to 6th\n",
    "        pooled = [hs.mean(dim=1).squeeze(0).cpu().numpy() for hs in hidden_states]\n",
    "        features = np.stack(pooled, axis=0)  \n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "    return features\n",
    "\n",
    "sys.stderr = open(os.devnull, 'w')\n",
    "def play_audio(file_path):\n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(file_path)  \n",
    "        data = waveform.cpu().numpy().astype(np.float32)\n",
    "\n",
    "        p = pyaudio.PyAudio()\n",
    "        stream = p.open(\n",
    "            format=pyaudio.paFloat32,\n",
    "            channels=data.shape[0],\n",
    "            rate=sample_rate,\n",
    "            output=True\n",
    "        )\n",
    "\n",
    "        interleaved = data.T.reshape(-1).tobytes()\n",
    "        stream.write(interleaved)\n",
    "\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error playing audio: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeAudioApp:\n",
    "    def __init__(self, master):\n",
    "        self.master = master\n",
    "        master.title(\"Fake Audio Detector\")\n",
    "        master.geometry(\"800x500\")\n",
    "        master.configure(bg=\"#f9f9f9\")\n",
    "\n",
    "        self.labels = data[\"labels\"]\n",
    "\n",
    "        style = ttk.Style(master)\n",
    "        style.theme_use('clam')\n",
    "        style.configure('TLabel', background='#f9f9f9', font=('Helvetica', 12))\n",
    "        style.configure('Header.TLabel', font=('Helvetica', 18, 'bold'))\n",
    "        style.configure('TButton', font=('Helvetica', 12), padding=6)\n",
    "\n",
    "        header = ttk.Label(master, text=\"Fake Audio Classification\", style='Header.TLabel')\n",
    "        header.pack(pady=(20, 5))\n",
    "\n",
    "        controls = ttk.Frame(master, padding=10)\n",
    "        controls.pack(fill='x')\n",
    "\n",
    "        ttk.Label(controls, text=\"Choose sample (1–10):\").grid(row=0, column=0, sticky='w')\n",
    "        self.number_var = tk.StringVar(value=\"1\")\n",
    "        number_menu = ttk.Combobox(controls, textvariable=self.number_var,\n",
    "                                   values=[str(i) for i in range(1, 11)],\n",
    "                                   state='readonly', width=5, font=('Helvetica', 12))\n",
    "        number_menu.grid(row=0, column=1, padx=10)\n",
    "\n",
    "        btn_frame = ttk.Frame(master, padding=(10, 10))\n",
    "        btn_frame.pack()\n",
    "\n",
    "        self.play_btn = ttk.Button(btn_frame, text=\"▶ Play\", command=self.play_selected, width=20)\n",
    "        self.play_btn.grid(row=0, column=0, padx=20)\n",
    "\n",
    "        self.classify_btn = ttk.Button(btn_frame, text=\"✔ Classify\", command=self.classify_selected, width=20)\n",
    "        self.classify_btn.grid(row=0, column=1, padx=20)\n",
    "\n",
    "        ttk.Separator(master, orient='horizontal').pack(fill='x', pady=15)\n",
    "\n",
    "        result_frame = ttk.Frame(master, padding=20)\n",
    "        result_frame.pack()\n",
    "\n",
    "        ttk.Label(result_frame, text=\"Prediction:\", font=('Helvetica', 14)).grid(row=0, column=0, sticky='w', padx=10)\n",
    "        self.result_lbl = ttk.Label(result_frame, text=\"–\", font=('Helvetica', 24, 'bold'), foreground='#007acc')\n",
    "        self.result_lbl.grid(row=1, column=0, padx=10)\n",
    "\n",
    "        ttk.Label(result_frame, text=\"Ground Truth:\", font=('Helvetica', 14)).grid(row=0, column=1, sticky='w', padx=40)\n",
    "        self.ground_lbl = ttk.Label(result_frame, text=\"–\", font=('Helvetica', 24, 'bold'), foreground='#444')\n",
    "        self.ground_lbl.grid(row=1, column=1, padx=40)\n",
    "\n",
    "        self.selected_file = None\n",
    "\n",
    "    def play_selected(self):\n",
    "        try:\n",
    "            idx = int(self.number_var.get()) - 1\n",
    "            self.selected_file = audio_files[idx]\n",
    "            play_audio(self.selected_file)\n",
    "        except Exception as e:\n",
    "            print(\"Playback Error\", str(e))\n",
    "\n",
    "    def classify_selected(self):\n",
    "        try:\n",
    "            idx = int(self.number_var.get()) - 1\n",
    "            self.selected_file = audio_files[idx]\n",
    "            waveform = load_audio(self.selected_file)\n",
    "            if waveform is None:\n",
    "                return\n",
    "\n",
    "            features = extract_features(waveform)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(features)\n",
    "                pred = torch.argmax(outputs, dim=1).item()\n",
    "                label = \"Fake\" if pred == 1 else \"Real\"\n",
    "\n",
    "            self.result_lbl.config(text=label)\n",
    "\n",
    "            gt = self.labels[idx]\n",
    "            gt_label = \"Fake\" if gt == 1 else \"Real\"\n",
    "            self.ground_lbl.config(text=gt_label)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Classification Error\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = FakeAudioApp(root)\n",
    "    root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
