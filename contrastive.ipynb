{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YNV1hguEaka"
      },
      "source": [
        "# **This is the complete code I used to implement the contrastive loss for the ASV including the code for the heatmap for the weights. It yielded an EER = 0.34% but almost all the weights were concentrated in layer 10.**\n",
        "\n",
        "# **Adding the contrastive loss required a change in the ResNetClassifier.forward(), Train(), and Test() functions. I used the MemmapDataset class for loading the dataset from .npy files. You can use the LayerFeatureDataset class on your pkl file.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paQUBJYjESS9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class LayerFeatureDataset(Dataset):\n",
        "    def __init__(self, pkl_path):\n",
        "        data = pickle.load(open(pkl_path, \"rb\"))\n",
        "        self.features = torch.tensor(data[\"features\"], dtype=torch.float32)\n",
        "        self.labels   = torch.tensor(data[\"labels\"],   dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "class MemmapDataset(Dataset):\n",
        "    def __init__(self, feat_path, lab_path):\n",
        "        self.X = np.load(feat_path, mmap_mode=\"r\")  # (N, L, D)\n",
        "        self.y = np.load(lab_path,   mmap_mode=\"r\")  # (N,)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.X[idx], dtype=torch.float32)  # (L, D)\n",
        "        y = int(self.y[idx])\n",
        "        return x, y\n",
        "\n",
        "# Models\n",
        "class LayerWeightedAggregator(nn.Module):\n",
        "    def __init__(self, num_layers):\n",
        "        super().__init__()\n",
        "        self.w = nn.Parameter(torch.ones(num_layers) / num_layers)\n",
        "    def forward(self, x):\n",
        "        # x: (batch, L, D)\n",
        "        w = torch.softmax(self.w, dim=0)               # (L,)\n",
        "        return (x * w[None, :, None]).sum(dim=1)       # (batch, D)\n",
        "\n",
        "class ResNetClassifier(nn.Module):\n",
        "    def __init__(self, num_layers, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.agg = LayerWeightedAggregator(num_layers)\n",
        "        H = W = int(np.sqrt(hidden_dim))\n",
        "        assert H * W == hidden_dim\n",
        "        self.H, self.W = H, W\n",
        "\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "        self.resnet.conv1 = nn.Conv2d(1,\n",
        "            self.resnet.conv1.out_channels,\n",
        "            kernel_size=self.resnet.conv1.kernel_size,\n",
        "            stride=self.resnet.conv1.stride,\n",
        "            padding=self.resnet.conv1.padding,\n",
        "            bias=False)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.agg(x)                        # (batch, D)\n",
        "        b = z.size(0)\n",
        "        img = z.view(b, 1, self.H, self.W)\n",
        "        logits = self.resnet(img)\n",
        "        return z, logits\n",
        "\n",
        "# Contrastive Loss\n",
        "def contrastive_loss(z1, z2, pair_labels, margin=1.0):\n",
        "    # z1, z2: (batch, D); pair_labels: 0=same, 1=different\n",
        "    z1 = F.normalize(z1, dim=1)\n",
        "    z2 = F.normalize(z2, dim=1)\n",
        "    dist = F.pairwise_distance(z1, z2)              # (batch,)\n",
        "    pos = (1 - pair_labels) * 0.5 * dist**2\n",
        "    neg = (    pair_labels) * 0.5 * torch.clamp(margin - dist, min=0.0)**2\n",
        "    return (pos + neg).mean()\n",
        "\n",
        "\n",
        "# Joint Training Loop\n",
        "def Train(model, optimizer, ce_criterion, train_loader, val_loader,\n",
        "          num_epochs=50, alpha=0.5, margin=1.0):\n",
        "    eer_per_epoch = []\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for feats, labels in train_loader:\n",
        "            feats, labels = feats.to(device), labels.to(device)\n",
        "\n",
        "            # Forward\n",
        "            z, logits = model(feats)\n",
        "            loss_ce = ce_criterion(logits, labels)\n",
        "\n",
        "            # Sample paired batch via random permutation\n",
        "            idx = torch.randperm(feats.size(0))\n",
        "            z2, labels2 = z[idx], labels[idx]\n",
        "            pair_lbl = (labels != labels2).float()\n",
        "\n",
        "            # Contrastive loss on embeddings\n",
        "            loss_con = contrastive_loss(z, z2, pair_lbl, margin)\n",
        "\n",
        "            # Combined loss\n",
        "            loss = loss_ce + alpha * loss_con\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # Validation: purely classification EER\n",
        "        model.eval()\n",
        "        all_labels, all_probs = [], []\n",
        "        with torch.no_grad():\n",
        "            for feats, labels in val_loader:\n",
        "                feats, labels = feats.to(device), labels.to(device)\n",
        "                z, logits = model(feats)\n",
        "                probs = torch.softmax(logits, dim=1)[:, 1]\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "        all_labels = np.array(all_labels)\n",
        "        all_probs  = np.array(all_probs)\n",
        "        fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "        fnr = 1 - tpr\n",
        "        eer = fpr[np.nanargmin(np.abs(fpr - fnr))]\n",
        "        eer_per_epoch.append(eer)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Val EER: {eer:.4f}\")\n",
        "\n",
        "    return eer_per_epoch\n",
        "\n",
        "def Test(model, test_loader, name=\"Model\"):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds  = []\n",
        "    all_probs  = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, labels in test_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            emb, outputs = model(features)\n",
        "\n",
        "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_preds  = np.array(all_preds)\n",
        "    all_probs  = np.array(all_probs)\n",
        "\n",
        "    # Classification metrics\n",
        "    accuracy  = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds)\n",
        "    recall    = recall_score(all_labels, all_preds)\n",
        "    f1        = f1_score(all_labels, all_preds)\n",
        "\n",
        "    # Manual FPR/FNR\n",
        "    TP = np.sum((all_preds == 1) & (all_labels == 1))\n",
        "    TN = np.sum((all_preds == 0) & (all_labels == 0))\n",
        "    FP = np.sum((all_preds == 1) & (all_labels == 0))\n",
        "    FN = np.sum((all_preds == 0) & (all_labels == 1))\n",
        "\n",
        "    fpr_manual = FP / (FP + TN) if (FP + TN) > 0 else 0.0\n",
        "    fnr_manual = FN / (FN + TP) if (FN + TP) > 0 else 0.0\n",
        "\n",
        "    # ROC & EER\n",
        "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fpr - fnr))\n",
        "    eer = fpr[eer_index]\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"=== Evaluation Metrics: {name} ===\")\n",
        "    print(f\"Accuracy : {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall   : {recall:.4f}\")\n",
        "    print(f\"F1 Score : {f1:.4f}\")\n",
        "    print(f\"FPR      : {fpr_manual:.4f}\")\n",
        "    print(f\"FNR      : {fnr_manual:.4f}\")\n",
        "    print(f\"EER      : {eer:.4f}\")\n",
        "    print(f\"AUC      : {roc_auc:.4f}\")\n",
        "\n",
        "    # Plot ROC\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.4f})')\n",
        "    plt.plot([0,1], [0,1], '--', label='Random')\n",
        "    plt.scatter(fpr[eer_index], tpr[eer_index], color='red',\n",
        "                label=f'EER = {eer:.4f}')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'{name} ROC Curve')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "###################################################################\n",
        "\n",
        "# Begin\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load dataset\n",
        "# use the LayerFeatureDataset class on a pkl features file: ds = LayerFeatureDataset(features_pkl)\n",
        "feat_path = \"/content/features.npy\"\n",
        "lab_path  = \"/content/labels.npy\"\n",
        "ds = MemmapDataset(feat_path, lab_path)\n",
        "\n",
        "\n",
        "\n",
        "# Train/Val/Test split (80/10/10)\n",
        "n = len(ds)\n",
        "train_len = int(0.8 * n)\n",
        "val_len   = int(0.1 * n)\n",
        "test_len  = n - train_len - val_len\n",
        "train_ds, val_ds, test_ds = random_split(ds, [train_len, val_len, test_len])\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False)\n",
        "\n",
        "# Info for model config\n",
        "num_layers  = ds.X.shape[1]\n",
        "hidden_dim  = ds.X.shape[2]\n",
        "num_classes = len(np.unique(ds.y))\n",
        "\n",
        "\n",
        "model = ResNetClassifier(num_layers, hidden_dim, num_classes)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "ce_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "eer_history = Train(\n",
        "    model, optimizer, ce_criterion,\n",
        "    train_loader, val_loader,\n",
        "    num_epochs=50,\n",
        "    alpha=0.5,       # weight for contrastive loss\n",
        "    margin=1.0       # contrastive margin\n",
        ")\n",
        "\n",
        "####################################################################\n",
        "\n",
        "# Testing\n",
        "\n",
        "Test(model, test_loader, name=\"ResNet18 Classifier\")\n",
        "\n",
        "###################################################################\n",
        "\n",
        "# Weights Heatmap\n",
        "w = model.agg.w.detach().cpu().numpy()\n",
        "\n",
        "# Apply softmax to the data to convert to probabilities\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Subtract max(x) for numerical stability\n",
        "    return exp_x / np.sum(exp_x)\n",
        "\n",
        "# Convert the raw data to probabilities via softmax\n",
        "data_softmax = softmax(np.array(w))\n",
        "\n",
        "# Reshape the data into a 1x24 matrix\n",
        "data_reshaped = data_softmax.reshape(1, -1)\n",
        "\n",
        "# Plot heatmap with the blue-to-red color scale and softmax probabilities\n",
        "plt.figure(figsize=(24, 1))  # Make the plot wide and short to fit the line of squares\n",
        "sns.heatmap(data_reshaped, annot=False, cmap='coolwarm', cbar=True, square=True, linewidths=0.5)\n",
        "\n",
        "# Adjust x-axis to start from 1 and remove the default labels\n",
        "plt.xticks(np.arange(0.5, 24.5), np.arange(1, 25), rotation=0)\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
